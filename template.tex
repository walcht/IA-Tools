% $Id: template.tex 11 2007-04-03 22:25:53Z jpeltier $
\documentclass{vgtc}                          % final (conference style)

\ifpdf%                                % if we use pdflatex
  \pdfoutput=1\relax                   % create PDFs from pdfLaTeX
  \pdfcompresslevel=9                  % PDF Compression
  \pdfoptionpdfminorversion=7          % create PDF 1.7
  \ExecuteOptions{pdftex}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.pdf,.png,.jpg,.jpeg} % for pdflatex we expect .pdf, .png, or .jpg files
\else%                                 % else we use pure latex
  \ExecuteOptions{dvips}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.eps}     % for pure latex we expect eps files
\fi%

%% it is recomended to use ``\autoref{sec:bla}'' instead of ``Fig.~\ref{sec:bla}''
\graphicspath{{figures/}{pictures/}{images/}{./}} % where to search for the images

\usepackage{microtype}                 % use micro-typography (slightly more compact, better to read)
\PassOptionsToPackage{warn}{textcomp}  % to address font issues with \textrightarrow
\usepackage{textcomp}                  % use better special symbols
\usepackage{mathptmx}                  % use matching math font
\usepackage{times}                     % we use Times as the main font
\renewcommand*\ttdefault{txtt}         % a nicer typewriter font
\usepackage{tabu}                      % only used for the table example
\usepackage{booktabs}                  % only used for the table 
\usepackage{url}
\usepackage{tabularray}
\usepackage[backend=biber]{biblatex}
\usepackage{hyperref}
\addbibresource{template.bib}

\onlineid{0}
\vgtccategory{Research}

\title{The State-of-Art Platforms and Tools for Immersive Analytical
Applications Development}

\author{Walid Chtioui\thanks{e-mail: walid.chtioui@ensi-uma.tn}\\ %
        \scriptsize University of Passau %
\and Achraf Hebheb\thanks{e-mail: habhabachref@gmail.com}\\ %
     \scriptsize University of Passau}

%% A teaser figure can be included as follows, but is not recommended since
%% the space is now taken up by a full width abstract.
%\teaser{
%  \includegraphics[width=1.5in]{sample.eps}
%  \caption{Lookit! Lookit!}
%}

\abstract{This paper presents an overview of the latest state-of-the-art platforms and toolkits which can be
used to develop immersive analytics (IA) experiences. Initially, a short overview of Unity3D and Unreal
Engine with a comparison of supported eXtended Reality (XR) features and third party tools is provided. After
that, we present an overview associated with a critique of what we believe are the major XR development
toolkits. For each toolkit we discuss the set of what we think are important features they provide or lack
such as the support for collaboration, interactions, real-time data and large datasets. Throughout the paper,
newly introduced toolkits are compared against previous ones in regards to performance, supported devices,
accessibility, and ease-of-use. A set of state-of-the-art tools and techniques, which can be used to extend
aforementioned toolkits, covering collaboration, interaction and
navigation topics are then provided. We then discuss two IA prototypes that provide
important insights through user feedback. We have come to the conclusion that,
although IA toolkits have come out of their infancy, we are still yet to see an
IA toolkit that provides a unified workflow allowing for versatile
visualizations, collaboration support, acceptable performance scalability,
real-time data support and support for a plethora of XR and non-XR devices
which has the potential to replicate the wide success of conventional
2D visualization frameworks such as D3.js.
} % end of abstract

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\firstsection{Introduction}
\maketitle

Immersive analytics was defined by Chandler et al. as "an emerging research thrust investigating how new
interaction and display technologies can be used to support analytical reasoning and decision making"
\cite{ia}. Recent technological breakthroughs and major competitors providing more affordable immersive head
mounted displays (hereafter HMD) have sparked further development into this field
\cite{web:vision_pro_unity}. This has in turn led to the rise of proposed IA experience development toolkits.
The latest survey of IA provided an exhaustive survey of IA works from 1991 to 2021 \cite{survey_of_ia}.
However, to the best of our knowledge, our paper is the first that delves into the technical details about
extensively used state-of-the-art IA platforms and suggested toolkits. We aim to identity the challenges
the IA community managed to address and the open challenges that we think are important for IA toolkits
to mimic the wide adoption of conventional 2D visualisations libraries. In particular, we want to provide
a quick overview of the currently available IA toolkits, what each of them lacks, and propose directions
to the IA community that we think are worth exploring.


\section{Methodology}

\noindent Our main goal is to analyse the literature to give an overview and a comparison of 
state-of-the-art IA toolkits and techniques. It is important to note that our objective is \textit{not} to
include the whole IA literature in the last four to six years. That would be way out of the scope of our
work. For literature selection, we have only used the \textit{IEEE Xplore Digital Library}. We have filtered
the search results to only include \textit{Conference} and \textit{Journal} types of papers. We have also filtered
the publication year to the range 2018 - 2024. We used the search terms \textit{immersive analytics},
\textit{immersive visualisation} which yielded 260, 1826 respectively. Results were sorted by
\textit{Relevance} and for each result we only went through the first 100 papers and picked what we thought
were relevant papers based on the title and the abstract. We then skimmed through these papers to Further
reduce their numbers by focusing more on toolkits and tools rather than prototypes.

\section{Platforms}

Platforms refer to general-purpose frameworks that provide a uniform workflow
allowing developers to create XR applications for a plethora of devices. Since
IA experiences and AR/VR gaming applications share a lot of similarities, game
engines provide excellent support for developing IA experiences. Two game
engines are considered; Unity and Unreal Engine. A particular focus is given
to Unity due to its wide adoption for XR applications.

\subsection{Unity}
Unity has a wide adoption in the world of XR thanks to its unified workflow
and support for various XR platforms. Unity supports an extensive set of XR
vendor-specific software development kits (hereafter SDK) including: Apple's
ARKit, Google's ARCore, Microsoft's HoloLens and OpenXR. Following the
announcement of Apple's mixed reality (MR) headset Vision Pro in Apple's
Worldwide Developers Conference (WWDC) 2023, Unity was announced to provide
native support for Vision's Pro operating system VisionOS \cite{web:vision_pro_unity}.
Unity also provides a set of XR packages that are built on top of these vendor
plugins to add application-level development tools \cite{unity:xr_packages}.
For instance, AR Foundation is an industry-standard framework that provides
support for various AR features such as: object tracking and plane detection.
Unity also provides XR Interaction Toolkit package which is a high-level,
component-based interaction system. The package also includes XR Device
Simulator which is a simulator that allows user input from conventional input
devices (a keyboard, a mouse or a controller) to drive XR headset and
controllers in the Unity scene view. This may be useful for debugging on a
wide range of XR devices without having to actually try them. Unity also
benefits from open-source external XR packages such as the Mixed Reality
Toolkit for Unity (MRTK) \cite{mrtk:repo} which is designed to further accelerate cross-platform MR development in Unity.

\subsection{Unreal Engine}

Unreal Engine is an industry-standard software for creating
hyper-photorealistic 3D/2D experiences. Thanks to its unified workflow across
numerous XR vendors such as Oculus, Microsoft, Vive, and PlayStation, it
provides the required assets to develop XR experiences This engine, like Unity,
comes with the core comprehensive collection of XR SDKs such as Google's
ARCore, Apple's ARkit, Magic Leap Microsoft's HoloLens, OpenXR, and Oculus.
However, Unreal Engine comes with a  less-than-ideal XR documentation relative
to Unity. This documentation does not provide comprehensive manuals for certain
XR features. As a result, we cannot guarantee that these functionalities are
supported. Moreover, Unreal Engine lacks official support for third-party XR
plugins as is the case with Vuforia.
\subsection{Comparison}
\autoref{table:1} provides a comparison between the previously discussed
platforms in terms of support for vendor-specific SDKs and a set of features.

\begin{figure}[t!]
	\centering
	\includegraphics[width=\columnwidth]{dxr_toolkit_00}
    \caption{On the left is a declarative specification file in JSON format. On the right is the resulting
    scatter plot visualization. In the \textit{data} attribute, the source input data is provided. The mark
    type is the built-in type \textit{cube}. Five visualization channels (or visual encodings) are used:
    two nominal data dimensions mapped to \textit{X} and \textit{Y} positions; ordinal time dimension mapped
    to \textit{Z} position; \textit{color} mapped to quantitative weight dimension; and \textit{size}
    represents the quantitative weight dimension.
    Image is taken from \cite{dxr:repo} and is available in the public domain.}
    \label{fig:dxr_toolkit_00}
\end{figure}

\begin{table*}
	\centering

	\begin{tabular}{l c c}
		\toprule
		                              & \multicolumn{2}{c}{\textbf{Platform}}                            \\
		\cmidrule(l){2-3}
		\textbf{SDKs}                 & Unity 2022 LTS                        & Unreal Engine 5.3        \\
		\midrule
		ARCore                        & X                                     & X                        \\
		ARKit                         & X                                     & X                        \\
		Magic Leap                    & X                                     & X                        \\
		Microsof HoloLens             & X                                     & X                        \\
		OpenXR                        & X                                     & X                        \\
		Oculus                        & X                                     & X                        \\
		WebXR                         & X                                     &                          \\
		VisionOS                      & X                                     &                          \\
		\midrule
		\textbf{AR-Specific Features} &                                       &                          \\
		\midrule
		Plane Detection               & X                                     & X                        \\
		Object Occlusion              & X                                     & X                        \\
		Environment Probes            & X                                     & X                        \\
		Face Tracking                 & X *(Android\& iOS only)               & X *(Android \& iOS only) \\
		Image Tracking                & X                                     &                          \\
		Object Tracking               & X *(iOS only)                         & ?                        \\
		Body Tracking                 & X *(iOS only)                         & ?                        \\
		Camera Intrinsics             & X                                     & X                        \\
		Meshing                       & X                                     & ?                        \\
		\midrule
		\textbf{XR Features}          &                                       &                          \\
		\midrule
		High-level XR Interactions    & X                                     & ?                        \\
		XR Input Simulation           & X                                     & ?                        \\
		\midrule
		Vuforia Support               & X                                     &                          \\
		\bottomrule
	\end{tabular}


	\medskip

	\caption{Per-platform supported SDKs, AR features and 3rd party tools. *?:
		feature could not be found in engine's official documentation.}
	\label{table:1}
\end{table*}

\section{Toolkits and Frameworks}

The term toolkit refers to development environments that are tailor-made for
IA experience development purposes. They provide, among other things,
high-level tools for authoring visualizations, built-in interactions and
support for multiple major XR devices.

\subsection{DXR Toolkit}

\noindent Sicat et al. proposed \textit{DXR} \cite{dxr_toolkit}; an open-source \cite{dxr:repo} IA toolkit
built on top of Unity. The toolkit provides fast prototyping and iteration for non-experienced users, i.e.,
users with little or no programming knowledge in XR and Unity. DXR targets AR/VR devices and provides highly
customizable data-points (hereafter referred to as \textit{marks}). 

\noindent The core design goals and features of DXR include:
\begin{itemize}
    \item \textbf{Authoring Using Vega-Lite-like Declarative Grammar} - Alongside the data input, DXR takes a
    specification file written in JavaScript Object Notation (hereafter \textit{JSON}) from which
    visualisations are created (\autoref{fig:dxr_toolkit_00}). The specification file is described in Vega-Lite declarative grammar \cite{vega_lite} - i.e., only what should be
    achieved has to be provided, not how. Therefore it is suitable for users with no programming experience to
    easily and rapidly realize IA experiences.
    \item \textbf{Providing Fast Idea-to-prototype Process} - DXR generates visualisations from the
    specification file at runtime. This allows for \textit{in-situ} authoring, i.e., during runtime while
    experiencing the immersive experience.
    \item \textbf{Providing Customizable Graph Marks} - A visualization object in DXR is a collection of
    Unity GameObjects, i.e., a Unity-specific entity that encapsulates an object's representation and its
    behaviour. The visualization properties of these GameObjects (e.g., color, size and rotation) are mapped
    to input data dimensions (hereafter we refer to such a mapping as a \textit{visualization channel}).
    This allows for the manipulation of data-points using standard Unity plugins which in turn results in
    extensive data-point customizability.
    \item \textbf{Providing Reusable Templates} - DXR also provides ready-to-use specification templates for
    common visualisations such as scatter plots and bar charts. This expands the scope of users to include
    those with little technical experience.
	\item \textbf{Providing a GUI for in-situ Authoring} - On top of the specification file, DXR also
    provides an Graphical User Interface (hereafter GUI) for an even easier and more templated in-situ
        authoring experience. This further broadens the range of users to include those with no technical knowledge.
\end{itemize}

\medskip

\noindent As the authors have explicitly mentioned, DXR is meant for prototyping and exploring designs, it is
not designed to handle visualisations of large datasets. On HoloLens, for datasets with more than
approximately a thousand item, suboptimal - i.e., less that 60 frames per second (hereafter FPS) -
performance has been observed. Nonetheless, the authors argue that DXR can still be useful for quickly and
cheaply prototyping large dataset designs before moving to more specialized and optimized implementations.

\medskip

\noindent Although DXR targets a wide range of users, the scope of its flexibility seems to be limited to
providing, among other things, custom graphical markers and custom visualization channels. That limits users
to a templated common set of visualisations such as scatter plots, bar charts and radial bars. There was also
no indication that DXR supports real-time data therefore limiting its usecase to offline data models only.

\noindent Although DXR provides a GUI to target absolute beginners, the initial setup still requires some
manipulation with the Unity editor. This in turn requires a Unity installation and a basic understand of the
Unity GUI. This could've been avoided by providing an initial runtime GUI where users can select a
specification file using a file browser in runtime or easily select a template from the GUI.

\noindent Moreover, further developments with new devices may be difficult to implement due to DXR's source
code not receiving any updates for more than six years \cite{dxr:repo}.

\subsection{IATK Toolkit}
Maxime et al. introduced IATK \cite{iatk_toolkit}; an open-source \cite{iatk:repo} software package for Unity
that provides both a high-level Unity-editor-integrated GUI for simple authoring and a low-level C-sharp
and JavaScript API for fine-grained authoring and extending the visualizations.

\smallskip

\noindent To some degree of similarity to DXR, IATK relies on a high-level
declarative grammar of graphics by providing a composable grammar of
visualization primitives alongside a high-level interface for rapid prototyping
and iterations. What sets IATK apart, is that it was designed with scalability
in mind, a focus on large and complex multidimensional datasets and a focus on
user interactions. The toolkit's authors claim that it can render millions of
items thanks to its use of efficient GPU shader code. Also, contrary to DXR,
IATK does not support declarative configurations, instead it relies on a Unity
editor GUI or C-sharp API code that make use of a composable grammar to author
visualizations.

\smallskip

\noindent Unlike DXR and other toolkits built on top of Unity, IATK does not
render the data-points (here we do not mean point as in a geometrical point but
as a representation of a data entry) as Unity game objects and use
expensive-to-update-at-large-scale object attributes as visualization channels.
Instead, all the data-points are visualized within one game object where each
data point is encoded into a unique vertex by mapping data attributes,
such as position, color and size, into vertex components such as vertex UV
coordinates, vertex normal vector and vertex color. This way, actual data-point
geometries are created on the GPU resulting in a potentially more
efficient rendering process. This however greatly limits the customizability
of data-point marks and the choice of visualization channels especially for
novice users.

\smallskip

\noindent According to the authors' performance statistics, on VR less than
90 FPS is observed at two million data-points and on AR less than 60 FPS is
observed at just a thousand data-points. However, one can subjectively claim
that the performance on AR HoloLens headset remains acceptable up-to ten
thousand data-points at which 41 FPS is achieved. Although the authors provided
FPS statistics for Oculus CV1, Meta2 and HoloLens devices, no performance
statistics were provided for the alternative game-object-based data-point
approach to provide a performance reference point.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{iatk}
	\caption[Caption for IATK]{IATK supported visualization types. (1) 3D
		connected dots. (2) 3D bar chart. (3) 3D Scatter plot. (4, 7) 3D
		Scatter plot matrix. (5) Parallel Coordinates Plots. (6) 2D Scatter plot
        matrix. (8) Linked visualizations. The images are from \cite{iatk:repo}
		and are in the public domain.}
	\label{fig:iatk_visualizations}
\end{figure}


\smallskip

\noindent IATK integrates an interactive visualization model within its
visualization components that allows a set of interactions including filtering,
brushing and linking (\autoref{fig:iatk_visualizations} (8)), details on demand,
animated transitions and attribute-based animations. These interactions are
implemented in the vertex Shader part of the rendering pipeline which leverages
the high parallelism nature of the GPU(s) making them particularly responsive
and efficient at handling large datasets.

\smallskip

\noindent IATK's high-level GUI provides just a small set of built-in visualization
types such as a 3D\textbackslash2D scatter plot (\autoref{fig:iatk_visualizations}
(3)), a parallel coordinates plot (\autoref{fig:iatk_visualizations} (5)) or a
3D\textbackslash2D scatter plot matrix (\autoref{fig:iatk_visualizations} (4, 6, 7)).
It also only provides a small set of data-point geometries. To extend these,
one has to use the provided low-level API which might limit expressiveness for
non-experienced users.

\smallskip

\noindent The authors did not mention any support for neither real-time data
visualization nor local or remote collaboration. It is also worth mentioning
that for VR, only Scatter plot visualization type is supported. This greatly
limits the expressiveness for VR users.

\subsection{VRIA Toolkit}
Peter et al. introduced VRIA \cite{vria_framework}; a free and open-source \cite{vria:repo}
framework for building IA experiences in VR. Unlike the aforementioned
toolkits, VRIA is not built as an add-on on top of a game engine. Instead, it is
built upon open-standard Web-based frameworks such as WebVR (now called WebXR instead), A-Frame, React and
D3.js. All of which are mature, open-source and widely used Web-based
frameworks and libraries. This allows VRIA applications to be accessible by a
plethora of VR and non-VR devices.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{vria_toolkit}
    \caption{Top: Examples of supported 3D/2D visualizations. Bottom: VRIA Builder.
    Image from \cite{vria:repo} and is available in the public domain.}
	\label{fig:vria_toolkit}
\end{figure}

\medskip

\noindent The core design goals and features of VRIA include:
\begin{itemize}
    \item \textbf{Employing a Vega-Lite-like Declarative Grammar} - Similar to DXR, VRIA makes use of a
    declarative grammar similar to Vega-Lite \cite{vega_lite} which allows users to create custom
    visualizations based on a configuration file. This makes the toolkit accessible to novice users.
    However, this visualization configuration file only provides support for basic functionalities.
    \item \textbf{Provides High Portability and Integrability} - Thanks to its composable structure, the
    toolkit can be integrated into other existing 2D or immersive visualization applications. For instance,
    VRIA's visualizations can be overlaid on top of another A-Frame scene. VRIA has support for collaborative
    immersive analytics through either the high-level networking abstraction layer provided by the
    open-source Networked A-Frame component or by using the provided API with lower-level networking
    libraries. 
    \item \textbf{Provides a Visualization Creation Tool} - VRIA provides The VRIA Builder; a Web application
    intended for beginners that integrates a GUI and a 3D scene view to rapidly prototype visualization
    designs with instant feedback without having to leave the browser. It is worth mentioning that, unlike
    DXR, this customization GUI is not part of the VR scene and cannot be viewed withing the VR headset.
    Only the scene view can be experienced immersively. However, the authors mentioned their willingness
    to add in-situ GUI so that users can build and prototype visualizations iteratively without having
    to remove the headset and switch back to desktop screen (\autoref{fig:vria_toolkit} (Bottom)).
    \item \textbf{Provides a Low-level API} - For extra custom functionalities such as a custom set of
    visualization channels, interactions, graphical marks or visualization types, more experienced users
    are advised to use the provided low-level API.
\end{itemize}

\medskip

\noindent In terms of data input, the toolkit only supports tabular data in the form of JSON or CSV thus
real-time data visualization is not supported. The authors expressed their willingness to add support for
other forms of data models such as geospatial GeoJSON, network, and relational models.

\medskip

\noindent The authors stated that VRIA offers the option to integrate D3.js \cite{d3_js} visualization within its IA
experiences. D3.js is web-base JavaScript visualization library that provides a low-level approach to author
graphics and powerful mechanisms to transform and manipulate data. Since D3.js is widely adopted, integrating
it within VRIA allows for easier usability and quicker authoring of 2D visualizations. However, there was no
mention of which D3.js functionalities and visualizations are supported nor any guidance on how to integrate
them within the VRIA framework.

\medskip

\noindent Furthermore, seasoned developers seeking for a more custom experience are forced to deal with two
separate tools to author custom visualizations; the visualization configuration file specified in a
declarative language and the API specified in JavaScript. We believe that it would be easier for the end
users if VRIA provided more customizability through the visualization configuration file instead of just
providing very basic functionalities. Although it is understandable that A-Frame simplifies the process of
creating VR experiences on the web relative to the lower level Three.js library, we question its necessity.
A-Frames' functionalities could be directly implemented in Three.js thus reducing and simplifying the VRIA
stack. The authors stated their intention to work at the Three.js level directly without the usage of any
further abstraction libraries.

\medskip

\noindent Whenever 3D graphics are mentioned in the context of web browsers,
performance implementations are one of the most worrying aspects. This is
especially true for Web-based VR applications where two images have to be
rendered each frame preferably at 90 FPS to avoid motion sickness. VRIA's
authors provided visualization benchmarks on a desktop monitor, a desktop
Oculus Rift CV1 and a smartphone. A scatter-plot visualization type was used
with sphere graphical marks. The exact set of visualization channels used
was not mentioned neither were attributes of the input data. On the desktop
monitor and Oculus Lift HMD, performance drops significantly after one
thousand data points. On smartphone performance starts dropping at one hundred
data points. This proves that VRIA is not suitable for large dataset
visualizations. The authors claim that the performance observed for the HMD
is similar to that of IATK for the HoloLens device. But the HoloLens uses
its own, much less powerful, hardware while VRIA performance benchmark used
the Oculus Lift CV1 VR HMD which relies on a separate desktop for expensive
rendering operations. We therefore question such comparisons of performance
benchmarks. This is later Web-based XR solutions do not compete,
in terms of performance, with game-engine based solutions.

\medskip

\noindent VRIA only supports Cartesian plots but the authors expressed their willingness to add support for
other coordinate systems including geographical and spherical coordinates. Moreover, although the authors
showcased the ease of integrating VRIA with AR.js to target AR experiences, the absence of an out-of-box
support for VRIA still limits AR accessibility only to experienced users.

\subsection{RagRug Toolkit}

\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{ragrug_example}
	\caption[Caption for RagRug]{Example of situated analytics using RagRug
		toolkit. Multiple visualizations are shown and are fed real-time temperature data from nearby IOT sensor devices.
        Images from \cite{ragrug:repo} and is in the public domain. }
	\label{fig:ragrug_example}
\end{figure}

Philipp et al. presented RagRug \cite{ragrug_toolkit}; An open-source
\cite{ragrug:repo} toolkit built on top of IATK
for \textit{situated analytics}, i.e., real-time data-fed interactive immersive
visualizations connected to a meaningful physical object in close proximity to
the user (hereafter we refer to such an object as a \textit{referent}).
For instance, a facility manager equipped with an AR HMD, when in proximity to
an air conditioner with multiple Internet-of-Things (IoT)
sensor devices monitoring temperature and other parameters, multiple real-time
visualizations pop up (\autoref{fig:ragrug_example}). Since it only targets
situated-analytics usecases, RagRug is an AR-only toolkit.
A major requirement for IA toolkits is to provide support for a wide variety of
input and output modalities spanning VR/AR and potentially conventional 2D
screen displays. RagRug aims, on top of that, to achieve a broad coverage of
IoT devices. It also intends to be used for rapid prototyping by providing
runtime interpretation of application logic.

\smallskip

\noindent The core design goals and features of RagRug include:
\begin{itemize}
	\item \textbf{Providing 3D Visual Encoding Capabilities} -
	      RagRug provides 3D \textit{visual encoding} capabilities, i.e.,
	      ways of mapping data dimensions into 3D visual structures. These
	      capabilities are inherited from IATK and the visual representations
	      are the same.
	\item \textbf{Making Visualizations Context Aware} -
	      RagRug provides visualizations that are context-aware with
	      regards to changes in the real world. This reduces the need to
	      explicitly specify every minute detail while the user is emerged in
	      a physical task. For instance, If a referent is moved, the attached
	      visualization will move along with it or if the lighting conditions
	      change then visualization color will adapt to such new condition
	      or if the user performs an invalid action, a notification is
	      adequately shown. RagRug makes use of IoT devices for collecting data
	      about the environment and a cloud server for, among other things, the
	      IoT application logic and depositing data in a database backend
	      so that the client can dynamically query it.
	\item \textbf{Supporting a Comprehensive Physical-Virtual Model} -
	      Information about the characteristics of referents such as location,
	      shape or identity are not hard-coded and are instead
	      queried from a suitable database. That way the client obtains
	      relevant data dynamically. This is important for correctly linking
	      visualizations to referents.
	\item \textbf{Making the Visualization Pipeline Reactive} -
	      The pipeline provided by RagRug is not re-evaluated after every external
          change. For instance, when new data comes, visualizations will be updated. Or
	      when a device is turned off, marks in a visualization will
	      change to reflect that. IATK lacks support for either real-time
	      data (since it expects a static dataset input) or for receiving
	      events from external sources. To address this, RagRug provides an
	      extension on top of IATK to reactively update data points on network
          events locally and externally.
	\item \textbf{Supporting Situated Authoring of Visualizations} -
	      Since testing situated analytics requires physical interactions with
	      referents, not allowing for situated authoring may result in a
	      cumbersome development experience. The authors propose runtime
	      interpretation of application logic which allows designers to modify
	      every aspect of the system without having to shut it down. To address
	      the lack of in-situ authoring from IATK, The authors make use of
	      PowerUI \footnote{https://github.com/Kulestar/powerui}, an
	      open-source JavaScript extension for Unity that allows for runtime UI
	      authoring using CSS, HTML and JavaScript.
\end{itemize}

\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{ragrug_stack}
	\caption[Caption for RagRug]{RagRug stack. (1) Data is acquired from IoT
		sensors using MQTT. (2) Data filtering. (3) Real-time data is sent to
		IATK as a response to a query. (4) Visualizations are
		rendered/updated in Unity.}
	\label{fig:ragrug_stack}
\end{figure}

\medskip

\noindent RagRug consists of two standalone platforms, one for experiencing immersive analytics
visualizations and interacting with them (referred to as \textit{client}) and one for IoT applications and
databases (referred to as \textit{hub}). As illustrated in (\autoref{fig:ragrug_stack}) the two
platforms make use of the widely adopted and lightweight MQTT \footnote{https://mqtt.org/} protocol to
communicate data and events. Another core design of the toolkit is its heavy reliance on a uniform
dataflow programming model. To achieve that, the hub makes use of Node-Red \footnote{https://nodered.org/};
a low-code programming tool for event-driven applications. The client also relies on the dataflow graphs of
Node-Red. These are provided to the client, alongside other JavaScript runtime dependencies, at
initialization time. The dataflow model expands the range of potential RagRug users to include those with
low programming experience. RagRug emphasises on making JavaScript its unified programming language by making
use of extensions that provide a JavaScript wrapper interface that calls the native interfaces under the hood
(e.g., PowerUI\footnote{https://powerui.kulestar.com/} which allows for runtime Unity UI authoring using
JavaScript instead of the native Csharp programming interface). Since the toolkit is event-driven, it
supports collaborative situated analytics.


\medskip

\noindent Although we do understand the importance of having a unified programming model, we find the use of
PowerUI, for the sole reason of providing a JavaScript interface for authoring Unity runtime UI,
questionable. Unity provides a variety of built-in runtime UI authoring tools that are mature,
better documented, and proven by the community \footnote{https://docs.unity3d.com/Manual/UI-system-compare.html}.
Unfortunately, these tools rely on Csharp for their logic which if adopted, the unified programming model is
no longer satisfied. Another concern is that PowerUI has
been deprecated from the Unity Asset store and the open-source GitHub repository has not been active, except
for a single text file change, for more than seven years \footnote{https://github.com/Kulestar/powerui}.
This may result in making further developments with new devices/software more difficult to implement.
We therefore argue for relaxing the constraint of a unified programming model in favor of a simpler 
technology-stack and reliance on established built-in tools.

\smallskip

\noindent We also believe that the reliance on low-code, visual-programming tools such as Node-Red can hinder
the scalability of RagRug. This would have not been an issue if RagRug provided a low-level API for users
looking for a more advanced, out-of-the-usual usecase.

\medskip

\noindent In-depth statistics for the performance of the RagRug toolkit was not provided. Instead, a single
statistic that used a HoloLens 2 as a client, a notebook computer as a hub and a 5GHz-band wireless network
was provided. However, the event propagation time was the only measured metric by this statistic.
This leaves the question of the usability of the toolkit in production environments with various data loads
largely unanswered.

\medskip

\noindent The authors did not provide any statistics about user feedbacks other than that they are mainly
researches. We believe that gathering feedback from a wide range of users, both client-side users and
developers, is important in determining the validity of the claimed characteristics of the toolkit (e.g.,
ease of usability, ease of customizability, etc.).

\subsection{Wizualization Toolkit}
\noindent Andreas et al. proposed Wizualization \cite{wizualization_toolkit}; an open-source \cite{wizualization:repo}
toolkit with a heavy focus on state-of-the-art multimodal speech, gesture, and touch interaction techniques.
Similar to VRIA, Wizualization is built on open-standard web technologies relying on WebXR standard for XR
input and output modalities and Web Speech API for speech capabilities. Wizualization relies on a magic
metaphor for authoring visualizations.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{wizualization_toolkit}
    \caption{Left: Wizualization's Spellbook containing predefined spells, selection tool for subsequent
    spells, and a history of executed spells. Middle: a collection of spells (mid-air gestures and spoken
    words). Right: Wizualization combines spell blocks to create Optomancy JSON specification. Images from
    \cite{wizualization_toolkit}.}
	\label{fig:wizualization_toolkit}
\end{figure}

\noindent The core design goals and features of Wizualization include:
\begin{itemize}
    \item \textbf{Support for Multimodal Input} - In Wizualization, different input modalities can
    accomplish the same task. This allows for a great flexibility especially since speech techniques may not
    be always appropriate \cite{limits_of_speech_recognition}.
    \item \textbf{Providing a Notebook of Predefined Gestures/Spoken Words} - Wizualization provides a
    runtime-accessible notebook and dictionary (\autoref{fig:wizualization_toolkit} (Left)) that provides a collection of mid-air gestures and spoken
    commands (hereafter we refer to such collection as \textit{spells}). This makes it a learning tool for
    available spells. It also provides a variable selection tool for subsequent spells and a history of used
    spells in the session.
    \item \textbf{Relying on a High Level Grammar of Graphics} - Wizualization introduced \textit{Optomancy};
    a set of grammar of graphics for the web that accepts a partial JSON specification file and compiles it
    into a full specification with sane defaults for missing properties (\autoref{fig:wizualization_toolkit} (Right)).
	\item \textbf{Supporting Remote and Co-located Collaboration} - Wizualization provides inherent
    asynchronous collaboration support where users can join a room and see existing state from previous
    sessions. Depending on the capabilities of the device they join with, users can even modify the scene.
    \item \textbf{Runtime Dataset Loading} - New datasets can be loaded at runtime within the same scene.
    \item \textbf{Support for Incremental Authoring} - Wizualization provides visual feedback for spells even
    if they only provide a partial specification to Optomancy.
\end{itemize}

\noindent The authors have not provided any empirical evidence to support the utility of the approaches
implemented by the toolkit. Furthermore, no performance statistics were provided which we think are a crucial
aspect of a 3D web-based application due to the limitations the web imposes.

\section{Prototypes}

While tools refer to systems that aim to allow developers to create IA
experiences, prototypes are applications that target end-users by providing IA
experiences attempting to solve particular IA-related and/or domain-specific
problems. Through the feedback of end-users, prototypes may provide interesting
insights on how IA tools should address particular challenges.


\subsection{Fiesta}

\noindent Benjamin et al. built FIESTA \cite{fiesta_prototype}; an open-source
\cite{fiesta:repo} team-based analysis prototype that allows users to create, manipulate and share data visualizations
in an unconstrained and physically co-located VR-only environment. The authors
state that the main objective behind developing FIESTA is to study how groups
collaboratively explore data in a free-roaming VR environment.

\medskip

\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{fiesta}
	\caption{(1) Real view of participants using the FIESTA system. (2)
		Top-down view of a virtual scene in which three participants are
		conducting visual analytics tasks. (3) A virtual view from the perspective of an
		observer. (4) Egocentric layout. (5) Planar layout. (6) Authoring panel
		with a faceted scatter plot on its right interface. (7) Details on
		demand to easily inspect data records. All images are taken from
		\cite{fiesta_prototype}.}
	\label{fig:fiesta}
\end{figure}

\noindent FIESTA is built on top of IATK. The authors justify the usage of IATK
for performance reasons since multiple users can each produce multiple
visualizations. A 16m\textsuperscript{2} (4m x 4m) room is used to conduct the
study. Each user is equipped with a VR headset and a backpack containing a
laptop to which the headset is connected (\autoref{fig:fiesta} (1)). To track the
headsets' positions, four trackers are positioned in the room; one in each
corner. An additional desktop PC manages the users' laptops using the Unity
networking engine Photon. Although the study focuses on physically co-located
IA environment, the authors state that the research prototype also supports
remote collaboration.

\medskip

\noindent The core design goals and features of FIESTA are:
\begin{itemize}
	\item \textbf{Baseline Visualizations} - The prototype provides three types
	      of 2D and 3D visualizations: scatter plots, faceted scatter plots
	      (\autoref{fig:fiesta} (6)) and line charts.
	\item \textbf{Baseline Interactions} - FIESTA relies on standard
	      interfacing techniques across desktop and VR. It provides grasping
	      techniques for interacting with UI elements at close range and ranged
	      pointing techniques using a laser pointer for long range
	      (\autoref{fig:fiesta} (7)).
	\item \textbf{Baseline Authoring} - Each user is provided with a movable
	      panel to author visualizations (\autoref{fig:fiesta} (2, 3, 4, 5, 6)).
	      The panel is split into two sections: on the left, a GUI is provided
	      to map data dimensions to visualization channels, choosing the
	      faceting dimension and adjusting the number of facets; the right
	      panel is used to show produced visualizations.
	\item \textbf{Tearing out Visualizations} - Visualizations can be cloned
	      from the panel by grasping and pulling them away. These cloned
	      visualizations can then be placed independently in the 3D space
	      (\autoref{fig:fiesta} (2, 4, 5)). Resizing and rescaling operations are
	      supported through grasping interactions on widgets placed along
	      visualization object's axis. Visualizations can also be brought back
	      into the panel for further editing or be destroyed through a
	      throwing-towards-ground interaction.
	\item \textbf{Linked Brushing} - Private and shared brushes are provided
	      for each user. The private brush allows for selections that are only
	      visible to the user while the shared brush allows for selections
	      which are visible to all users. Brushing is linked across multiple
	      visualizations through color channel, i.e. contrary to other tools
	      where lines are used to visualize linked data-points.
	\item \textbf{Pointer and Details on Demand} - A pointer is provided and is
	      activated through controller's trigger. Details-on-demand tool of the
	      nearest data-point is enabled through a touchpad input
	      (\autoref{fig:fiesta} (7)). The pointer can also be used to interact
	      with the panel's interface through point-and-click interaction for
	      buttons or point-and-drag interaction for sliders and pickers.
	\item \textbf{Free Roaming Shared Environment} - To emulate a physically
	      co-located collaborative environment, Each user is represented by a
	      uniquely colored avatar and a floating nameplate
	      (\autoref{fig:fiesta} (3)). The same unique color of the user's avatar
	      is used for coloring shared brush selection. There is no ownership
	      system in FIESTA; everything is seen and interactive by all users
	      except the private brush tool.
	\item \textbf{Room and Surface Affordances} - To explore how users
	      naturally use surface-based functionalities in collaborative IA
	      environments to solve visual analytics tasks, a four-wall room with
	      a square tabletop in the center is used. The virtual walls act as a
	      boundary to prevent users from hitting the actual room walls.
	      Releasing visualizations close to the virtual surface of the table
	      top make them rest on top of it.
\end{itemize}

\medskip

\noindent A total of 30 participants were recruited for the study with all but
one having a background in computer science. The participants managed to
perform visual analytics tasks and remained engaged with these tasks for 45
minutes on average. This proves that collaborative visual analysis is possible
in VR. Participants made use of 3D visualizations although their use was not
emphasized. They came up with interesting ways to view them such as: scaling a
3D visualization to room scale and standing in the middle of it; laying 3D
visualizations around them in an egocentric layout (\autoref{fig:fiesta} (4)) and
looking at them from an axis-aligned viewpoint. With 2D visualizations,
participants preferred placing them in a grid-like layout on the walls. This
made them more presentable to others at all times contrary to 3D visualizations
where users struggled to take the perspective of observers into consideration
when viewing them. Therefore, IA tools should explore methods to facilitate the
presentation of 3D visualizations to other users. Another interesting
observation is that 3D visualizations heavily influenced the way 2D
visualizations are organized. For instance, in the presence of 3D
visualizations, these 2D visualizations were placed in an egocentric layout.
These observations demonstrate the importance of a consistent UI design that
supports both 2D and 3D visualizations. The study findings may also justify
the need to provide different placement tools for 2D and 3D visualizations.
Furthermore, although the usage of surfaces is completely optional in FIESTA,
participants made use of the walls to neatly place their 2D visualizations.
However, they saw no benefit in using the central table top for visualization
placements. This demonstrates that users are influenced by environment
configuration only if there is a tangible benefit to its usage. The majority
of users mixed between individually and collaboratively working although both
modes were optional in FIESTA. This proves that providing support for both
working modes is invaluable. The study also showed that participants did not
interact with objects that, except in cases where such interactions were
expected, did not belong to them. However, the reason behind that could be
avoiding physical collisions with other users. Therefore a study with remotely
co-located environment is needed to further study these behaviours.

\medskip

\noindent Although the usage of surfaces is optional in FIESTA, they were
inherently predefined - the users did not have the option to arrange such
surfaces. A similar system in which they have to ability to define their own
surfaces may further expand these findings. Furthermore, FIESTA is a VR-only
prototype therefore these findings may not be entirely applicable to IA experiences
where AR is used.


\subsection{Uplift}
Barrett et al. proposed Uplift \cite{uplift_prototype}; an in-place
collaborative visual analytics prototype targeting users with diverse
expertise in the domain of micro-grids. Uplift is designed for casual visual
analysis use-cases; i.e. to be used to easily identify, in a relatively short
time, key patterns in complex visualized data. The requirements for the
prototype were initially provided and subsequently modified, through multiple
feedback sessions, by a wide range of stakeholders including micro-grid project
and energy systems experts.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{uplift}
	\caption[Caption for Uplift]{A showcase of Uplift prototype. (1)
		Translucent scaled-down models of campus buildings placed on top of the
		colored tabletop to reflect energy consumption. (2) Users gathering
		around the central tabletop supported with a large 2D display. (3)
		A user picking up a scaled-down building model of the campus. (4)
		A user interacting with a tangible widget affecting a slider which
		in turn affects time granularity and therefore the number of slices
		in (5). (5) AR for visualizing data above and around the central
		tabletop, in particular hourly energy consumption is visualized as 2D
		slices laid over buildings. (6) A picked-up campus scaled-down building
		model overlaid with relevant data by AR.
		Images from \cite{uplift_prototype}.}
	\label{fig:uplift}
\end{figure}

\medskip

\noindent The main design goals of Uplif that were identified through multiple
stakeholders feedback sessions are:
\begin{itemize}
	\item \textbf{Walk Up and Use}: The system should be inviting, intriguing and
	      compelling.
	\item \textbf{Low Technical Barrier to Entry}: Easily usable by users with a wide
	      degree of technical knowledge.
	\item \textbf{Supports Multiple Participants}: Provide flexible collaboration
	      support to small and large groups alike.
	\item \textbf{Supports Analytical Sprints}: Provide support for short analytical
	      activities.
\end{itemize}

\medskip

\noindent Uplift relies more than just AR headsets to bring casual
collaborative visual analytics to a multitude of micro-grid stakeholders.
A tabletop display showing a geographical map of the campus grid is used as a
central platform where users are supposed to gather around and interact with
widgets placed on top of it (\autoref{fig:uplift} (2)). Uplift also makes use of
tangible widgets which are physical and interactive elements that control
visualization parameters (for instance by affecting sliders) (\autoref{fig:uplift}
(4)). The prototype also relies on scaled-down physical models of buildings
that are translucent which allows the color of the surface on which they are
placed to be used as an appealing visualization channel
(\autoref{fig:uplift} (1)). If these models are picked up, AR will overlay
relevant data about them (\autoref{fig:uplift} (6)). AR is used to display
multiple 3D data types on top of the tabletop and 2D graphs alongside legends
around it (\autoref{fig:uplift} (5)). On top of these, Uplift uses a large display
to either replicate the content of the tabletop or show additional
visualizations (\autoref{fig:uplift} (2)).

\medskip

\noindent Through the feedback of 16 participants who tried the prototype,
Uplift was proven to be potentially useful for micro-grid-related data
analytics.

\medskip

\noindent Although Uplift was designed for micro-grid-related systems,
the authors claim that its applicability domain can be extended to include
other fields that rely on analysis of complex spatial data for sense-making
such as the construction industry. However, the use of a wide range of technologies
and gadgets makes Uplift a specialized solution that we believe is not yet
ready for wide deployment. For instance, on top of using Vuforia for tabletop
tracking, Uplift uses the motion capture Vicon system
\footnote{https://www.vicon.com/} with a proprietary tracking software and four
cameras to track the tangible widgets and the buildings. However,
since no proof was provided for the usefulness of overlaying information when
a building model is picked, we question the usage of object tracking for this
purpose. This is particularly true if such feature requires the addition of
expensive gadgets and proprietary software. For instance, model position is
already embedded in the tabletop map visualization and that can be used by AR
to overlay visualizations above without the need of object tracking. We also
argue that the use of tangible widgets to control visualization parameters does
not justify the usage of additional gadgets and software required to track
them.

\medskip

\noindent Uplift expects a static data input therefore, although the topic of
real-time monitoring of the microgrid was seen as beneficial for operators by
expert stakeholders, the tool does not support real-time immersive analytics.

\section{Immersive Analytics Tools and Techniques}
\subsection{Collaboration Tools and Techniques}
\subsection{Interaction Tools and Techniques}

\noindent Interaction is an important component of information visualization. It has even been argued that it provides
a way to overcome the limits of representation and augment a user's cognition \cite{interaction_infovis}.

\smallskip

\noindent In the field of information visualization (\textit{Infovis}), interactions have been divided into seven
categories: \textbf{Select}. User marks something as interesting; \textbf{Explore}. User instructs the
infovis system to represent different/more data (e.g., a different subset of the data);
\textbf{Reconfigure}. User instructs the infovis system to change the arrangement/perspective of the
representation; \textbf{Encode}. User changes visual properties of data-points; \textbf{Abstract/Elaborate}.
User requests more or less information about data-points (e.g., a tooltip); \textbf{Filer}. User instructs the
system to show data-points based on some criteria; \textbf{Connect}. User instructs the system to show related
items. Some of these interactions can be combined together in a single interaction tool (e.g., \textit{Select}
and \textit{Connect} in \textit{linked brushing} tool).

\smallskip

\noindent Alongside these IA-specific interactions, text input can be an important aspect of a good IA
experience. This is especially the case for data manipulation tasks where, for instance, a user may be
expected to enter a textual information based on which a dataset is filtered.

\noindent Although researchers have experimented with speech interaction techniques
\cite{wizualization_toolkit}, these methods suffer from limitations concerning privacy, noise sensitivity (
e.g., in in-place collaborative environments), speech errors correction, cognitive interference, and
inappropriateness in certain environments \cite{limits_of_speech_recognition}.

\noindent Researchers experimented with the use of physical keyboards and touchscreens for text input
in VR. Particularly, Jens et al. \cite{keyboard_vs_touchscreen} provided an analysis conducted with 24
participants that compared desktop keyboard performance with that of touchscreen. Results have proven that
users retain more of their typing speed when using desktop keyboards.

These results are consistent with previous findings where standard physical keyboards
(QWERTY layout) have consistently scored the highest average words-per-minute (\textit{WPM}) speed and the
lowest average error rate compared to other types of physical input methods \cite{physical_text_entry_comparison}.

\noindent However, a conventional physical keyboard requires a stationary VR environment which is not necessarily the
case for IA environments. To address this issue, virtual keyboards are suggested as an alternative. Kern et
al. \cite{tap_vs_word_gesture_keyboards} provided a user evaluation with 64 participants, the majority of
which are students, to compare between contact-based mid-air virtual tap and swipe keyboards for
non-stationary VR and video see-through (VST) AR (\autoref{fig:swipe_keyboard}). The study reported a
significantly higher user experience ratings for the tap method in both non-stationary VR and VST AR. It also
reported that the type of XR display (e.g., VR, VST AR, etc.) significantly influences typing performance
with both input methods being considerably faster in VR than in VST AR.

\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{swipe_keyboard}
    \caption{Example of a virtual swipe keyboard in a VR context. The user intends to enter the German word
    \textit{deutsch}. To do so, they \textit{swipe} starting from the letter \textit{d} through all
    intermediate letters and ending the swipe at the letter \textit{h}. By using an error correction
    algorithm and a language model, the intended word is guessed. Image is taken
    from\protect\footnotemark and is available in the public domain.}
    \label{fig:swipe_keyboard}
\end{figure}

\footnotetext{https://www.youtube.com/watch?v=qxsF2XDHLag}

\medskip

\noindent To the best of our knowledge, research on the influence of haptic feedback for virtual input
methods seems to be lacking. We believe such research is important in giving potentially new insights about
improving user typing speed and user experience.

\section{Transition Tools and Techniques}
Mohammad et al. introduced HybridAxes \cite{hybridaxes_tool}; an extension of
IATK toolkit that allows for a smooth interoperability between 2D desktop
visualizations and immersive reality experiences (\autoref{fig:hybridaxes}).

\medskip

\begin{figure}[tb]
	\centering
	\includegraphics[width=\columnwidth]{hybridaxes}
	\caption[Caption for RagRug]{HybredAxes context switch processes. (1, 2, 3)
		Pulling data/visualization from a 2D screen. (4, 5, 6) Pushing a
		visualization to a 2D screen from an immersive AR environment.}
	\label{fig:hybridaxes}
\end{figure}

\noindent HybridAxes aims to provide a balanced feature set and performance
between the two ends of the Reality-Virtuality continuum while giving the users
the ultimate choice to choose which environment to work in. To avoid
disruptions of the user's flow when context switching, the tool also aims to
provide a smooth transition by reducing visualization generation delays. It
claims to do that by anticipating the user's interactions in both modes and
preemptively generating
visualizations in the background. Moreover, the authors stated that providing
a clear signaling of transition status either through visual (\autoref{fig:hybridaxes} (2, 5)) or
controller-related feedback is a core design requirement for the tool.
Keeping the same values for the visualization channels, if possible, when
transitioning a visualization between the two experiences is also stated as
a core design goal. HybridAxes should also provide support for synchronized
cross-virtuality brushing and linking. For instance, a user may have a tabular
data on a 2D desktop alongside a 3D scatter-plot in the immersive environment.
When highlighting an entry in the table, associated data-points in the
scatter-plot should also be highlighted. HybridAxes, aims to also favor desktop
for interactions that require the detailed manipulation of text entries. The
choice of interaction method, either through free-hand or
controller-based interactions, is given to the user.

\medskip

\noindent In terms of implementation, HybridAxes adopts CODAP; an open-source
\footnote{https://codap.concord.org} visual analytics tool, alongside a plugin
that enables the desktop system to receive commands via a Websocket interface.
A Node.js-based Websocket server is used to relay the messages between Unity
and the desktop system. We question the use of these extra tools and frameworks
since we believe that transitioning between immersive and 2D Unity
visualizations is superior to the suggested workflow. Our main argument is
that this way all visualizations are established within one framework - Unity -
which therefore allows for a more versatile, accessible and easier-to-use
toolkit. The tool provides support for the same set of visualizations as IATK
mainly: Scatter plots, Parallel Coordinate Plots and Bar Charts.

\medskip

\noindent In terms of performance, the authors claim that anticipating user actions has resulted in smoother
experience. However, no comparative performance measurements were provided.

\smallskip

\noindent In terms of interactions, three types are supported:
\begin{itemize}
	\item \textbf{Pull visualizations from the 2D desktop} - by using free-hand gestures
	      or the controller(s) (\autoref{fig:hybridaxes} (1, 2, 3)).
	\item \textbf{Push visualizations from the immersive environment back to the 2D
		      desktop} (\autoref{fig:hybridaxes} (4, 5, 6)).
	\item \textbf{Brush data-points on either side of the environments and see their
		      counterparts in the other side get highlighted}.
\end{itemize}


\noindent The authors hypothesize that enabling visual analytics users to
smoothly context switch between a 2D screen and 3D immersive experience could
improve their sense-making abilities. A limited study of the usefulness of the
tool has been conducted on just four participants where they analyzed the data
in three scenarios: 1) just using 2D desktop screen, 2) using a hybrid
desktop/AR system and 3) just using the AR system. The authors claim that the
study results align with their hypothesis. However, neither metrics nor
sufficient details were provided to prove this claim. Nonetheless, the authors
mentioned their willingness to perform a more rigorous study in the future.

\section{Discussion}

Throughout our survey, we have learned that current state-of-the-art IA
toolkits have, to some degree, properly addressed a set of important
challenges. These challenges include:

\begin{itemize}
	\item \textbf{Uniform Workflow} - Proposed toolkits managed to provide
	      uniform streamlined workflows that made it easier for users to make
	      IA experiences.
	\item \textbf{Ease-of-use} - IA community provided tools for both: novice
	      users with little or no XR and programming experience; expert users
	      to allow them to expand the toolkit. The community also experimented
	      a lot with high-level grammars.
	\item \textbf{Representation} - Community provided 2D and 3D visualizations
	      for common plots such as: scatter-plots, bar charts and parallel
	      coordinates plots. In particular, visual representation of connected
	      data-points between plots was also focused on.
	\item \textbf{Interaction} - Community managed to provide interaction tools
	      that handle, among other things, repositioning, resizing, selection,
	      details-on-demand, brushing and filtering. These are essential for
	      better immersive analysis experience.
	\item \textbf{Collaboration} - Community provided valuable insights about
	      the behavior of participants in collaborative IA environment both
	      in co-located space and remotely. The community also experimented
	      with potential new features and documented their usefulness. These
	      features include revisiting interaction history, user avatars and
	      shared brushing tools.
	\item \textbf{Adequate Authoring Tools} - Adequate authoring tools for
	      visualizations in the form of GUIs or/and configuration files were
	      provided.
	\item \textbf{Low-level API for Extensions} - In most discussed toolkits,
	      a low-level API allowing for the extension of functionalities is
	      provided. This has allowed newer toolkits and prototypes to build upon a
          customized version of a previous toolkit
          (e.g., \cite{ragrug_toolkit,fiesta_prototype,hybridaxes_tool}).
\end{itemize}

\noindent Although the aforementioned toolkits and prototypes prove that the
field of immersive analytics is past its infancy stage, there are still many
open challenges that are not yet properly or not at all addressed. We believe
that support for the following features by current state-of-the-art IA toolkits
is still lacking:

\begin{itemize}
	\item \textbf{Versatile Data Input Model} - Community should keep
	      assumptions about the input data as minimal as possible. In
	      particular, real-time data should be supported. Moreover, methods for
	      efficiently updating visualizations should be adopted as well (e.g.,
	      when a new data-point is received, when visual characteristics of some
	      data-points change, etc.).
	\item \textbf{In-depth Performance Statistics} - Community should focus
	      more on addressing the core challenge of performance. Convincing and extensive
	      performance statistics are still lacking.
	\item \textbf{Simple Technology Stack} - Community should focus more on
	      simplifying the technology stack and avoiding characteristics that
	      may hinder the wide adoption of IA toolkits (e.g., usage of many
	      tools from different vendors, usage of no-longer-maintained third
	      party tools, usage of expensive or hard-to-get hardware, etc.).
	\item \textbf{Usage of Open-Standard Technologies} - Community should focus more on
	      using open-standard frameworks/libraries. Particularly, we do believe 
	      that the community should experiment more with the low-level XR library
	      OpenXR instead of game-development frameworks.
	\item \textbf{Clear Licensing Terms} - For a successful wide adoption,
	      licensing terms for toolkits should be clarified and complex licenses
	      should be avoided.
	\item \textbf{Extensive Customization Support} - Community has, to some degree,
	      successfully addressed the customizability aspect of IA toolkits.
	      Nonetheless, we argue in favor of providing a low-level API that
	      allows for a wider range of configurations.
	\item \textbf{Built-in Collaboration Support} - Community should work more towards
	      adding built-in collaboration support (i.e., treat collaboration as a first-class citizen).
          Moreover, aspects of collaboration should be customizable (e.g., user avatars, shared tools, etc.).
	\item \textbf{Portability to Conventional 2D Screens} - Community should
	      work more towards adding support, on top of XR technologies, to
	      conventional 2D screens. We believe this is important since XR
          devices are still not yet as widely adopted as conventional 2D displays.
	\item \textbf{Portability to the Web} - Since web-based conventional visualization libraries have seen a
          wide adoption (e.g, \cite{d3_js}), the community should work more on adding web support.
	\item \textbf{In-situ Authoring Tools} - Community should work more on adding in-place (i.e., in real
          use-case environment) authoring tools for easier and faster prototyping.
\end{itemize}

\noindent We do therefore argue in favor of a visualization-tailored tool that is built on top of
open-standard XR technologies. In particular, we do firmly believe that an open-source and free-to-use -
i.e., provides a permissive free software license - tool built on top of the open-standard OpenXR API can
mimic the success of conventional 2D visualization libraries such as D3.js \cite{d3_js}. Unfortunately, we
have not come across such a framework and the closest to that in terms of reliance on open-standard
technologies would be VRIA and Wizualization toolkits \cite{vria_framework, wizualization_toolkit}.

\section{Conclusion}
We presented an overview of the platforms, tools and prototypes in the last six years which can be used to
develop IA experiences. Although IA tools have certainly come out of their infancy, we have come to the
conclusion that there is not yet any IA toolkit that has the same level of versatility,
visualization-tailored, ease-of-use and support of various data inputs and streams such as the widely
successful 2D visualization libraries like D3.js \cite{d3_js}. This is consistent with the latest IA survey
from 2021 \cite{survey_of_ia} where there was no mention of any tool that meets these criteria.

\printbibliography
\end{document}
